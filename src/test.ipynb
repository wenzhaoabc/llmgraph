{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-11T02:47:29.848065800Z",
     "start_time": "2024-08-11T02:47:28.700442500Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path[0] = sys.path[0] + \"/../\"\n",
    "\n",
    "from src.prompts import SYSTEM_PROMPT, ALIGN_PROMPT, DEEP_RELS, QA_PROMPT, CONTINUE_PROMPT, LOOP_PROMPT, MERGE_PROMPT\n",
    "from src.llm import LLM\n",
    "from src.text_extract import (\n",
    "    get_nodes_relationships_from_rawtext,\n",
    "    nodes_rels_combine_text,\n",
    "    merge_nodes_rels,\n",
    ")\n",
    "from src.tools import encode_image\n",
    "from src.dataclass import Chunk, Relationship, Entity, Image\n",
    "from src.multimodal.img import (\n",
    "    extract_images,\n",
    "    extract_entity_rels_images,\n",
    "    merge_entity_rels,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    stream=open(\"log.log\", \"w\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def split_document(\n",
    "        document: str, chunk_size: int = 1000, over_lap: float = 0.1\n",
    ") -> list[str]:\n",
    "    from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "    text_splitter = MarkdownTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=int(chunk_size * over_lap)\n",
    "    )\n",
    "    chunks = text_splitter.split_text(document)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _extract_images_paths(document: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    提取文档中的图片路径和图片标题。\n",
    "    \"\"\"\n",
    "    images = re.findall(r\"!\\[.*?\\]\\((.*?)\\)\", document)\n",
    "    return images\n",
    "\n",
    "class LLMForEntityExctract:\n",
    "    def __init__(self, initial_system_prompt):\n",
    "        self.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": initial_system_prompt},\n",
    "        ]\n",
    "        self.llm = LLM()\n",
    "\n",
    "    def add_message_and_call_llm(self, new_message, model=\"gpt-4o-mini\"):\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": new_message})\n",
    "        res = self.llm.chat(self.conversation_history, callback=None, model=model)\n",
    "        # assistant_message = res[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # self.conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": res})\n",
    "        return res\n",
    "\n",
    "def extract_entities_relations(\n",
    "        chunk: Chunk, prompt\n",
    ") -> tuple[list[\"Entity\"], list[\"Relationship\"]]:\n",
    "    \"\"\"\n",
    "    使用LLM提取chunk中的实体及关系。\n",
    "    \"\"\"\n",
    "\n",
    "    max_gleanings = 2   # 最大迭代次数\n",
    "    # messages = [\n",
    "    #     {\"role\": \"system\", \"content\": prompt},\n",
    "    #     {\"role\": \"user\", \"content\": chunk.text},\n",
    "    # ]\n",
    "    # llm = LLM()\n",
    "    # # callback = lambda x: print(x, end=\"\")\n",
    "    # res = llm.chat(messages, callback=None, model=\"gpt-4o-mini\")\n",
    "    # logging.info(f\"Extracted entities and relationships from chunk.llm res:\\n {res}\")\n",
    "\n",
    "    llm = LLMForEntityExctract(prompt)\n",
    "    res = llm.add_message_and_call_llm(chunk.text)\n",
    "    logging.info(f\"Extracted entities and relationships from chunk.llm: init iteration res:\\n {res}\")\n",
    "\n",
    "    for i in range(max_gleanings):\n",
    "        res = llm.add_message_and_call_llm(CONTINUE_PROMPT)\n",
    "        logging.info(f\"Extracted entities and relationships from chunk.llm: {i} iteration res:\\n {res}\")\n",
    "        # results += response.output or \"\"\n",
    "\n",
    "        # if this is the final glean, don't bother updating the continuation flag\n",
    "        if i >= max_gleanings - 1:\n",
    "            break\n",
    "\n",
    "        continue_res = llm.add_message_and_call_llm(LOOP_PROMPT)\n",
    "        if continue_res != \"YES\":\n",
    "            break\n",
    "\n",
    "    nodes_rels = get_nodes_relationships_from_rawtext(res)\n",
    "    entities = [Entity.from_dict(node) for node in nodes_rels[\"nodes\"]]\n",
    "    relationships = [Relationship.from_dict(rel) for rel in nodes_rels[\"relationships\"]]\n",
    "    for entity in entities:\n",
    "        if entity.properties.get(\"references\"):\n",
    "            entity.references = entity.properties[\"references\"]\n",
    "            del entity.properties[\"references\"]\n",
    "\n",
    "    for rel in relationships:\n",
    "        if rel.properties.get(\"references\"):\n",
    "            rel.references = rel.properties[\"references\"]\n",
    "            del rel.properties[\"references\"]\n",
    "\n",
    "    \"\"\"\n",
    "    images: list[Image] = extract_images(chunk)\n",
    "    images = _extract_images_paths(chunk.text)\n",
    "    merged_node_rels = nodes_rels\n",
    "\n",
    "    for img in images:\n",
    "        logging.info(f\"Processing image: {img}\")\n",
    "        content = (\n",
    "            IMAGE_PROMPT\n",
    "            + \"\\nBelow is a list of entities and relationships extracted from the context of the image:\\n\"\n",
    "            + nodes_rels_combine_text(merged_node_rels)\n",
    "        )\n",
    "        base64_image = encode_image(img)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": content},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": base64_image},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        logging.info(f\"Sending image to LLM messages: {messages}\")\n",
    "        images_nodes_rels_text = llm.chat(messages, callback=None, model=\"gpt-4o-mini\")\n",
    "        logging.info(\n",
    "            f\"Extracted entities and relationships from image.llm res:\\n {images_nodes_rels_text}\"\n",
    "        )\n",
    "        image_nodes_rels = get_nodes_relationships_from_rawtext(images_nodes_rels_text)\n",
    "        for node in image_nodes_rels[\"nodes\"]:\n",
    "            if node[\"properties\"][\"images\"]:\n",
    "                node[\"properties\"][\"images\"].append(img)\n",
    "            else:\n",
    "                node[\"properties\"][\"images\"] = [img]\n",
    "        for rel in image_nodes_rels[\"relationships\"]:\n",
    "            if rel[\"properties\"][\"images\"]:\n",
    "                rel[\"properties\"][\"images\"].append(img)\n",
    "            else:\n",
    "                rel[\"properties\"][\"images\"] = [img]\n",
    "        merged_node_rels: dict[str, list] = merge_nodes_rels(\n",
    "            merged_node_rels, image_nodes_rels\n",
    "        )\n",
    "    \"\"\"\n",
    "    return entities, relationships\n",
    "\n",
    "\n",
    "def align_entities_relations(\n",
    "        previous_result: dict[str, list], current_chunk: dict[str, list]\n",
    ") -> dict[str, list]:\n",
    "    \"\"\"\n",
    "    将上一个chunk的结果与当前chunk的结果一起发送给LLM，要求其完成实体及关系对齐。\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "            \"Previous entities and relationships:\\n\"\n",
    "            + nodes_rels_combine_text(previous_result)\n",
    "            + \"\\nCurrent entities and relationships:\"\n",
    "            + nodes_rels_combine_text(current_chunk)\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ALIGN_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    llm = LLM()\n",
    "    res = llm.chat(messages, callback=None, model=\"gpt-4o-mini\")\n",
    "    logging.info(f\"Aligned entities and relationships.llm res:\\n {res}\")\n",
    "    return get_nodes_relationships_from_rawtext(res)\n",
    "\n",
    "\n",
    "def dig_deep_relationships(nodes_rels: dict[str, list]):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DEEP_RELS},\n",
    "        {\"role\": \"user\", \"content\": nodes_rels_combine_text(nodes_rels)},\n",
    "    ]\n",
    "    llm = LLM()\n",
    "    llm_res = llm.chat(messages, callback=None, model=\"gpt-4o-mini\")\n",
    "    logging.info(f\"Deep relationships.llm res:\\n {llm_res}\")\n",
    "    return get_nodes_relationships_from_rawtext(llm_res)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T02:48:12.839916300Z",
     "start_time": "2024-08-11T02:48:12.811344400Z"
    }
   },
   "id": "d003c70c91ccc2a9",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22456\\.conda\\envs\\LlamaParse-Env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"\n",
    "    Use the BERT model to generate the embeddings\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T02:48:20.866775600Z",
     "start_time": "2024-08-11T02:48:13.611032800Z"
    }
   },
   "id": "a4ec3671563a5676",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "def build_k_nearest_graph(entities, k=5):\n",
    "    \"\"\"\n",
    "    Build the KNN Graph\n",
    "    \"\"\"\n",
    "    embeddings = np.array([get_text_embedding(entity.name) for entity in entities])\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='cosine').fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "\n",
    "    graph = {}\n",
    "    for i, entity in enumerate(entities):\n",
    "        graph[entity.name] = []\n",
    "        for idx in indices[i]:\n",
    "            if idx != i:  # Not connected to self\n",
    "                graph[entity.name].append((entities[idx], distances[i][indices[i] == idx][0]))\n",
    "    return graph\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T02:48:30.001142Z",
     "start_time": "2024-08-11T02:48:25.878640500Z"
    }
   },
   "id": "4a620f2e54e8a63b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def filter_weakly_connected_components(graph, distance_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Filter weakly connected\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for entity, neighbors in graph.items():\n",
    "        for neighbor, distance in neighbors:\n",
    "            if distance < distance_threshold:\n",
    "                G.add_edge(entity, neighbor.name)\n",
    "\n",
    "    weak_components = list(nx.connected_components(G))\n",
    "    return weak_components\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T02:52:44.418948300Z",
     "start_time": "2024-08-11T02:52:43.968000300Z"
    }
   },
   "id": "254bafe4a4394b94",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def merge_entities(entities, components):\n",
    "    \"\"\"\n",
    "    Merge entities\n",
    "    \"\"\"\n",
    "    merged_entities = []\n",
    "    original_to_merged_map = {}  # record the mapping of the old name to the new name\n",
    "\n",
    "    # Convert entities list to a dic\n",
    "    if isinstance(entities, list):\n",
    "        entities = {entity.name: entity for entity in entities}\n",
    "\n",
    "    processed_entities = set()  # record processed entities\n",
    "    \n",
    "    for component in components:\n",
    "        entity_names = list(component)\n",
    "        content = \"Should the following entities be merged?\\n\" + \"\\n\".join(entity_names)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": MERGE_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "        ]\n",
    "        llm = LLM()\n",
    "        llm_res = llm.chat(messages, callback=None, model=\"gpt-4o-mini\")\n",
    "        logging.info(f\"llm_res: {llm_res}\")\n",
    "        print(f\"llm_res: {llm_res}\")\n",
    "        if llm_res.strip().lower() == \"yes\":\n",
    "            # merge entity\n",
    "            primary_entity = entities[entity_names[0]]\n",
    "            for name in entity_names[1:]:\n",
    "                entity = entities[name]\n",
    "                primary_entity.references.extend(entity.references)\n",
    "                primary_entity.properties.update(entity.properties)\n",
    "                primary_entity.images.extend(entity.images)\n",
    "                # record mapping of old name to new name\n",
    "                original_to_merged_map[name] = primary_entity.name\n",
    "                # record processed entities\n",
    "                processed_entities.add(name)\n",
    "\n",
    "            original_to_merged_map[primary_entity.name] = primary_entity.name \n",
    "            merged_entities.append(primary_entity)\n",
    "            processed_entities.add(primary_entity.name)\n",
    "        else:\n",
    "            # not merged\n",
    "            for name in entity_names:\n",
    "                original_to_merged_map[name] = name\n",
    "                processed_entities.add(name)\n",
    "            merged_entities.extend([entities[name] for name in entity_names])\n",
    "\n",
    "    # entities not in components\n",
    "    for entity_name, entity in entities.items():\n",
    "        if entity_name not in processed_entities:\n",
    "            merged_entities.append(entity)\n",
    "            original_to_merged_map[entity_name] = entity_name\n",
    "    return merged_entities, original_to_merged_map\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T03:32:47.770103900Z",
     "start_time": "2024-08-11T03:32:47.745946800Z"
    }
   },
   "id": "fc89146cfc2620f7",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def update_relationships(relationships, merged_entities, original_to_merged_map):\n",
    "    \"\"\"\n",
    "    Update relationships\n",
    "    \"\"\"\n",
    "    name_map = {entity.name: entity for entity in merged_entities}\n",
    "\n",
    "    updated_relationships = []\n",
    "    for rel in relationships:\n",
    "        # use original_to_merged_map to find updated entity name\n",
    "        start_name = original_to_merged_map.get(rel.start, rel.start)\n",
    "        end_name = original_to_merged_map.get(rel.end, rel.end)\n",
    "\n",
    "        # find entity through name\n",
    "        start = name_map.get(start_name, start_name)\n",
    "        end = name_map.get(end_name, end_name)\n",
    "\n",
    "        # create updated relationships\n",
    "        updated_relationships.append(Relationship(\n",
    "            start=start.name if isinstance(start, Entity) else start,\n",
    "            end=end.name if isinstance(end, Entity) else end,\n",
    "            type=rel.type,\n",
    "            references=rel.references,\n",
    "            properties=rel.properties,\n",
    "            images=rel.images\n",
    "        ))\n",
    "\n",
    "    return updated_relationships\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T03:31:01.981197700Z",
     "start_time": "2024-08-11T03:31:01.971191100Z"
    }
   },
   "id": "5354d407d0d91722",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "entities = [\n",
    "    Entity(name=\"Company A\", label=\"Company\", references=[\"ref1\"], properties={\"location\": \"NY\"}, images=[\"image1\"]),\n",
    "    Entity(name=\"Company B\", label=\"Company\", references=[\"ref2\"], properties={\"location\": \"SF\"}, images=[\"image2\"]),\n",
    "    Entity(name=\"Company_B\", label=\"Company\", references=[\"ref3\"], properties={\"location\": \"SF\"}, images=[\"image3\"]),\n",
    "    Entity(name=\"Company C\", label=\"Company\", references=[\"ref4\"], properties={\"location\": \"LA\"}, images=[\"image4\"]),\n",
    "    Entity(name=\"NYC\", label=\"City\", references=[\"ref5\"], properties={\"population\": \"8M\"}, images=[\"image5\"]),\n",
    "    Entity(name=\"New York City\", label=\"City\", references=[\"ref6\"], properties={\"population\": \"8M\"}, images=[\"image6\"]),\n",
    "]\n",
    "\n",
    "relationships = [\n",
    "    Relationship(start=\"Company A\", end=\"Company B\", type=\"partnership\", references=[\"rel_ref1\"], properties={}, images=[]),\n",
    "    Relationship(start=\"Company_B\", end=\"Company C\", type=\"acquisition\", references=[\"rel_ref2\"], properties={}, images=[]),\n",
    "    Relationship(start=\"NYC\", end=\"New York City\", type=\"connection\", references=[\"rel_ref3\"], properties={}, images=[]),\n",
    "]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T02:55:47.802923700Z",
     "start_time": "2024-08-11T02:55:47.778811800Z"
    }
   },
   "id": "dfa28a97db4ebcbc",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build knn graph\n",
    "graph = build_k_nearest_graph(entities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T02:56:02.400788100Z",
     "start_time": "2024-08-11T02:56:02.084385700Z"
    }
   },
   "id": "998a82bbeaa10a3a",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[(Entity(name='Company C', label='Company', references=['ref4'], properties={'location': 'LA'}, images=['image4']),\n  0.038917303),\n (Entity(name='Company A', label='Company', references=['ref1'], properties={'location': 'NY'}, images=['image1']),\n  0.20522702),\n (Entity(name='Company_B', label='Company', references=['ref3'], properties={'location': 'SF'}, images=['image3']),\n  0.21149576),\n (Entity(name='NYC', label='City', references=['ref5'], properties={'population': '8M'}, images=['image5']),\n  0.34885496)]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph['Company B']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T03:34:19.430333500Z",
     "start_time": "2024-08-11T03:34:19.386131500Z"
    }
   },
   "id": "96f3092a823d825f",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Filter weakly connected components\n",
    "components = filter_weakly_connected_components(graph)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T02:57:15.592850Z",
     "start_time": "2024-08-11T02:57:15.575229400Z"
    }
   },
   "id": "9fbd847fe5355915",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'Company A', 'Company B', 'Company C'}]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T02:57:22.940396Z",
     "start_time": "2024-08-11T02:57:22.910286100Z"
    }
   },
   "id": "21f9a7b092020bba",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_res: no\n"
     ]
    }
   ],
   "source": [
    "merged_entities, original_to_merged_map = merge_entities(entities, components)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T03:32:55.070966500Z",
     "start_time": "2024-08-11T03:32:52.503896400Z"
    }
   },
   "id": "bf5284877f2a840b",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Entity(name='Company B', label='Company', references=['ref2'], properties={'location': 'SF'}, images=['image2']),\n Entity(name='Company C', label='Company', references=['ref4'], properties={'location': 'LA'}, images=['image4']),\n Entity(name='Company A', label='Company', references=['ref1'], properties={'location': 'NY'}, images=['image1']),\n Entity(name='Company_B', label='Company', references=['ref3'], properties={'location': 'SF'}, images=['image3']),\n Entity(name='NYC', label='City', references=['ref5'], properties={'population': '8M'}, images=['image5']),\n Entity(name='New York City', label='City', references=['ref6'], properties={'population': '8M'}, images=['image6'])]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_entities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T03:33:01.440007800Z",
     "start_time": "2024-08-11T03:33:01.434991600Z"
    }
   },
   "id": "83ee5492e030124f",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Company B': 'Company B',\n 'Company C': 'Company C',\n 'Company A': 'Company A',\n 'Company_B': 'Company_B',\n 'NYC': 'NYC',\n 'New York City': 'New York City'}"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_to_merged_map"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T03:33:03.078326300Z",
     "start_time": "2024-08-11T03:33:03.063427200Z"
    }
   },
   "id": "bda24526261c7c4f",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "updated_relationships = update_relationships(relationships, merged_entities, original_to_merged_map)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T03:33:34.207950Z",
     "start_time": "2024-08-11T03:33:34.196392100Z"
    }
   },
   "id": "68c8b6b9b8cf92e9",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Relationship(start='Company A', end='Company B', type='partnership', references=['rel_ref1'], properties={}, images=[]),\n Relationship(start='Company_B', end='Company C', type='acquisition', references=['rel_ref2'], properties={}, images=[]),\n Relationship(start='NYC', end='New York City', type='connection', references=['rel_ref3'], properties={}, images=[])]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_relationships"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T03:33:37.952968900Z",
     "start_time": "2024-08-11T03:33:37.917899200Z"
    }
   },
   "id": "e3d83f0d69b0a5dc",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c0fa57b208ba6fb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
